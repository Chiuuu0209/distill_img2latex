seed: 1234

trainer:
  gpus: 1
  overfit_batches: 0.0
  check_val_every_n_epoch: 10
  fast_dev_run: false
  max_epochs: 50
  min_epochs: 1
  num_sanity_val_steps: 0
  auto_lr_find: false
  checkpoint_callback: true

callbacks:
  model_checkpoint:
    save_top_k: 1
    save_weights_only: true
    mode: "min"
    monitor: "val/loss"
    filename: "{epoch}-{val/loss:.2f}-{val/cer:.2f}"
  early_stopping:
    patience: 100
    mode: "min"
    monitor: "val/loss"
    min_delta: 0.001

data:
  batch_size: 8
  num_workers: 4
  pin_memory: false
  BS: False             # IF use beamsearch dataset

lit_model:
  # Optimizer
  lr: 0.001
  weight_decay: 0.0
  # Scheduler
  milestones: [10]
  gamma: 0.5
  # Model
  d_model: 128
  dim_feedforward: 256
  nhead: 4
  dropout: 0.3
  num_decoder_layers: 3
  max_output_len: 500
  # pretrained weight
  teacher: True
  pretrained_weight :   # your pretrained_weight
  # loss
  loss: "soft"          # "soft" or "hard"
  embedding: True       # True or False
  temperature: 5        # knowledge distillation temperature -> recommanded 3~20
  # ratio of different loss
  r_target : 0.33
  r_soft : 0.33
  r_hard : 0.33
  r_embedding : 0.33
logger:
  project: "Distill-im2latex"
  save_dir : # your save path
  id: "Target_Soft_Embed"
